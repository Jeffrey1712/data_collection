{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Yelp\n",
    "\n",
    "The aim of this exercise is to allow a user to make an automatic search on <a href=\"https://www.yelp.fr/\" target=\"_blank\">Yelp</a> and store the results in a `.json` file. You will be guided through the different steps: making a form request with search keywords, parsing the search results, crawling all the result pages and storing the results into a file.\n",
    "\n",
    "‚ö† **As scrapy is not made to launch several crawler processes in the same script, you will have to restart your notebook's kernel before completing each question!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.11.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (42.0.4)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (24.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (6.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (65.5.0)\n",
      "Requirement already satisfied: packaging in /Users/jeffreyensenat/Library/Python/3.11/lib/python/site-packages (from scrapy) (23.2)\n",
      "Requirement already satisfied: tldextract in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (5.1.1)\n",
      "Requirement already satisfied: lxml>=4.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (5.1.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Requirement already satisfied: pyasn1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
      "Requirement already satisfied: pyasn1-modules in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
      "Requirement already satisfied: automat>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (4.9.0)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (3.6)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: six in /Users/jeffreyensenat/Library/Python/3.11/lib/python/site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a class `YelpSpider(scrapy.Spider)` with `start_urls = ['https://www.yelp.fr/']`. In this class, define a `parse(self, response)` method that automatically fills Yelp's homepage form with : \"restaurant japonais\" as search keywords and \"Paris\" as search location. Then, define another method `after_search(self, response)` that parses the first page of results, and yields the name and url of each search result. Finally, declare a `CrawlerProcess` that will store the results in a file named `\"restaurant_japonais-paris.json\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/Users/jeffreyensenat/Documents/python_fullstack/data_collection/Scrapy_advanced/02-Optional_Scraping_Yelp/yelp1.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python 02-Optional_Scraping_Yelp/yelp1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Once you've managed to get the first page's results in `restaurant_japonais-paris.json`, complete the `after_search(self,response)` method to crawl the different result pages, such that all the search results will be stored in the file `\"restaurant_japonais-paris.json\"`. Restart your notebook's kernel, execute the new `CrawlerProcess` and check that all the search results (and not only the first page) are now stored in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/Users/jeffreyensenat/Documents/python_fullstack/data_collection/Scrapy_advanced/02-Optional_Scraping_Yelp/yelp2.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python 02-Optional_Scraping_Yelp/yelp2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 16:12:54 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-02-26 16:12:54 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.4, Platform macOS-13.6.4-x86_64-i386-64bit\n",
      "2024-02-26 16:12:54 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-02-26 16:12:54 [scrapy.extensions.telnet] INFO: Telnet Password: 63c0da59350b6504\n",
      "2024-02-26 16:12:54 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-02-26 16:12:54 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/110.0.5481.78'}\n",
      "2024-02-26 16:12:54 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-02-26 16:12:54 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-02-26 16:12:54 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-02-26 16:12:54 [scrapy.core.engine] INFO: Spider opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 16:12:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-02-26 16:12:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Start the crawling using the spider you defined above\u001b[39;00m\n\u001b[1;32m     55\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(YelpSpider)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scrapy/crawler.py:429\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[1;32m    426\u001b[0m     reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup\u001b[39m\u001b[38;5;124m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_shutdown\n\u001b[1;32m    428\u001b[0m     )\n\u001b[0;32m--> 429\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/twisted/internet/base.py:693\u001b[0m, in \u001b[0;36mReactorBase.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/twisted/internet/base.py:930\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signals\u001b[38;5;241m.\u001b[39muninstall()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class YelpSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"yelp\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.yelp.fr/']\n",
    "\n",
    "    # Parse function for form request\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to make a search in Paris\n",
    "        # https://www.yelp.fr/search?find_desc=restaurant+japonais&find_loc=Paris\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={\n",
    "                'find_desc': 'restaurant japonais',\n",
    "                'find_loc':'Paris'\n",
    "                },\n",
    "            callback=self.after_search\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "    def after_search(self, response):\n",
    "            # en cliquant sur le nom de plusieurs restaus : li[1], li[2] donc li\n",
    "        names = response.xpath('//*[@id=\"main-content\"]/div/ul/li/div[1]/div/div/div[2]/div[1]/div[1]/div[1]/div/div/h3/span/a/text()')\n",
    "       \n",
    "        urls = response.xpath('//*[@id=\"main-content\"]/div/ul/li/div/div/div/div[2]/div[1]/div[1]/div[1]/div/div/h3/span/a')\n",
    "        \n",
    "        for name, url in zip(names,urls):\n",
    "            yield {\n",
    "                'name': name.get(),\n",
    "                'url': url.attrib[\"href\"]\n",
    "            }\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"page1_japonais-paris.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "#if filename in os.listdir('exo/'):\n",
    "        #os.remove('exo/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/110.0.5481.78',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {'Scrapy_advanced/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(YelpSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 16:10:45 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-02-26 16:10:45 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.4, Platform macOS-13.6.4-x86_64-i386-64bit\n",
      "2024-02-26 16:10:45 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-02-26 16:10:45 [py.warnings] WARNING: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2024-02-26 16:10:45 [scrapy.extensions.telnet] INFO: Telnet Password: 78c58733a458f85e\n",
      "2024-02-26 16:10:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-02-26 16:10:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/121'}\n",
      "2024-02-26 16:10:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-02-26 16:10:45 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-02-26 16:10:45 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-02-26 16:10:45 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-02-26 16:10:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-02-26 16:10:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Start the crawling using the spider you defined above\u001b[39;00m\n\u001b[1;32m     64\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(YelpSpider)\n\u001b[0;32m---> 65\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scrapy/crawler.py:429\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[1;32m    426\u001b[0m     reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup\u001b[39m\u001b[38;5;124m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_shutdown\n\u001b[1;32m    428\u001b[0m     )\n\u001b[0;32m--> 429\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/twisted/internet/base.py:693\u001b[0m, in \u001b[0;36mReactorBase.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/twisted/internet/base.py:930\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signals\u001b[38;5;241m.\u001b[39muninstall()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class YelpSpider(scrapy.Spider):\n",
    "    name = \"yelp\"\n",
    "    start_urls = ['https://www.yelp.fr/']\n",
    "\n",
    "    # Parse function for form request\n",
    "\n",
    "    def parse(self, response):\n",
    "        # FormRequest used to make a search in Paris\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={\n",
    "                'find_desc': 'restaurant japonais',\n",
    "                'find_loc':'Paris'\n",
    "                },\n",
    "            callback=self.after_search\n",
    "        )\n",
    "\n",
    "    # Callback used after login\n",
    "\n",
    "    def after_search(self, response):\n",
    "        \n",
    "        names = response.xpath('//*[@id=\"main-content\"]/div/ul/li/div/div/div/div[2]/div[1]/div[1]/div[1]/div/div/h3/span/a/text()')\n",
    "        urls = response.xpath('//*[@id=\"main-content\"]/div/ul/li/div/div/div/div[2]/div[1]/div[1]/div[1]/div/div/h3/span/a')\n",
    "        \n",
    "        for name, url in zip(names,urls):\n",
    "            yield {\n",
    "                'name': name.get(),\n",
    "                'url': url.attrib[\"href\"]\n",
    "            }\n",
    "            \n",
    "            # Select the NEXT button and store it in next_page\n",
    "        try:\n",
    "            next_page = response.xpath('//*[@id=\"main-content\"]/div/ul/li[13]/div/div[1]/div/div[11]/span/a').attrib[\"href\"]\n",
    "\n",
    "        except KeyError:          \n",
    "            logging.info('No next page. Terminating crawling process.')\n",
    "        \n",
    "        else:\n",
    "            yield response.follow(next_page, callback=self.after_search)\n",
    "\n",
    "        \n",
    "# Name of the file where the results will be saved\n",
    "filename = \"all_japonais-paris.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "#if filename in os.listdir('exo/'):\n",
    "        #os.remove('exo/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/121',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'Scrapy_advanced/' + filename: {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(YelpSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you've just made the proof of concept of making an automated search on Yelp with Scrapy! Now, let's improve the script such that it will allow the user to make any search at any location üòé\n",
    "\n",
    "3. Use python's `input()` function to ask the user which keywords and location he would like to use, and save them into two variables: `search_keywords` and `search_location`. Then, change the `parse(self, response)` method such that it fills Yelp's form with user-defined keywords and location. Finally, change the `CrawlerProcess` such that it stores the results in a file named with the following format : `search_keywords-location.json`. \n",
    "\n",
    "Try your search engine with different keywords and locations ‚úåÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in the terminal to be able to interract\n",
    "```shell\n",
    "python 02-Optional_scraping_yelp/yelp3.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a script that will use the json file you just created to create a list of urls you wish to scrape and then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/FULL_STACK_12_WEEK_PROGRAM/M04-Data_Collection_and_Management/D02-Web_Scraping/03-Instructors/01-Solutions'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open(\"02-Optional_Scraping_Yelp/restaurant_japonais-paris.json\")\n",
    "file = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.fr//biz/sanukiya-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/sushi-yaki-paris-4?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/onigiriya-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/aki-paris-2?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/okuda-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/y-izakaya-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/ippudo-paris-2?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/la-maison-du-sak%C3%A9-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/teppanyaki-ginza-onodera-paris?osq=restaurant+japonais',\n",
       " 'https://www.yelp.fr//biz/ginza-paris-5?osq=restaurant+japonais']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_urls = [\"https://www.yelp.fr/\" + element[\"url\"] for element in file]\n",
    "list_urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Scrape the list of urls and gather the following data about each restaurant (or place):\n",
    "    * name\n",
    "    * stars\n",
    "    * number of votes\n",
    "    * address\n",
    "    * opening hours\n",
    "    * phone\n",
    "    * amenities\n",
    "    * reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-18 17:39:16 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-03-18 17:39:16 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.1, Platform Linux-5.4.144+-x86_64-with-glibc2.31\n",
      "2022-03-18 17:39:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True, 'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-03-18 17:39:16 [scrapy.extensions.telnet] INFO: Telnet Password: e155f1a4f63028fc\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.throttle.AutoThrottle']\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-03-18 17:39:16 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-03-18 17:39:16 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-03-18 17:39:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:39:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-03-18 17:40:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:41:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:42:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-03-18 17:42:29 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/sanukiya-paris?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/sanukiya-paris?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "2022-03-18 17:42:36 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/sushi-yaki-paris-4?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:37 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/sushi-yaki-paris-4?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "2022-03-18 17:42:44 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/onigiriya-paris?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/onigiriya-paris?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "2022-03-18 17:42:50 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.yelp.fr//biz/aki-paris-2?osq=restaurant+japonais> (failed 3 times): 503 Service Unavailable\n",
      "2022-03-18 17:42:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <503 https://www.yelp.fr//biz/aki-paris-2?osq=restaurant+japonais>: HTTP status code is not handled or not allowed\n",
      "^C\n",
      "2022-03-18 17:42:55 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \n",
      "2022-03-18 17:42:55 [scrapy.core.engine] INFO: Closing spider (shutdown)\n"
     ]
    }
   ],
   "source": [
    "!python 02-Optional_Scraping_Yelp/yelp4.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbc4d3870518eee81184ced0d2279c769a0eca59aab465c4e7ec13e5e6c47a3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
