{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRObT5dxtgu0"
   },
   "source": [
    "# Become a movie director\n",
    "\n",
    "Let's use Scrapy to get some information about the 250 top rated movies on <a href=\"http://www.imdb.com/\" target=\"_blank\">IMDB</a>.\n",
    "\n",
    "1. Install `Scrapy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23Mz67dMtgu4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.11.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (42.0.4)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (24.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (6.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (65.5.0)\n",
      "Requirement already satisfied: packaging in /Users/jeffreyensenat/Library/Python/3.11/lib/python/site-packages (from scrapy) (23.2)\n",
      "Requirement already satisfied: tldextract in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (5.1.1)\n",
      "Requirement already satisfied: lxml>=4.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (5.1.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Requirement already satisfied: pyasn1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
      "Requirement already satisfied: pyasn1-modules in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
      "Requirement already satisfied: automat>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (4.9.0)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (3.6)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: six in /Users/jeffreyensenat/Library/Python/3.11/lib/python/site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a folder where you will store the scripts and results of this exercise. Create a file named `imdb1.py` in that folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Start by writing code in the script to:\n",
    "* import os, logging, scrapy, and CrawlerProcess from scrapy.crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a first spider called `imdb_spider` with:\n",
    "* name `imdb`\n",
    "* start_urls `https://www.imdb.com/chart/top`\n",
    "\n",
    "This spider should scrape the ranking, the title, the url, the crew, the rating and the number of voters for the first movie of the charts.\n",
    "\n",
    "Set up the `CrawlerProcess`.\n",
    "Save the results to a `imdb1.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 17:52:44 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2024-02-23 17:52:45 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.11.2 (v3.11.2:878ead1ac1, Feb  7 2023, 10:02:41) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.4, Platform macOS-13.6.4-x86_64-i386-64bit\n",
      "2024-02-23 17:52:45 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2024-02-23 17:52:45 [scrapy.extensions.telnet] INFO: Telnet Password: 2dc68d6fd8b40927\n",
      "2024-02-23 17:52:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2024-02-23 17:52:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2024-02-23 17:52:45 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2024-02-23 17:52:45 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2024-02-23 17:52:45 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2024-02-23 17:52:45 [scrapy.core.engine] INFO: Spider opened\n",
      "2024-02-23 17:52:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2024-02-23 17:52:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6029\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Start the crawling using the spider you defined above\u001b[39;00m\n\u001b[1;32m     53\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(imdb_spider)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scrapy/crawler.py:429\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[1;32m    426\u001b[0m     reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup\u001b[39m\u001b[38;5;124m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_shutdown\n\u001b[1;32m    428\u001b[0m     )\n\u001b[0;32m--> 429\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/twisted/internet/base.py:693\u001b[0m, in \u001b[0;36mReactorBase.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/twisted/internet/base.py:930\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signals\u001b[38;5;241m.\u001b[39muninstall()\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class imdb_spider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"imdb\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'https://www.imdb.com/chart/boxoffice',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    # It will get text, author and tags of the first <div> with class=\"quote\"\n",
    "    def parse(self, response):\n",
    "        return {                      #/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div/a/h3\n",
    "            \"ranking\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div/a/h3/text()').get().split(\".\")[0]\n",
    "            ,\n",
    "                                    #/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div/a/h3\n",
    "            \"title\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div/a/h3/text()').get().split(\".\")[1].strip(\" \")\n",
    "            ,                     #                   \n",
    "            \"url\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/div/a').attrib[\"href\"]\n",
    "            ,                                #/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/ul/li[2]/span[2]\n",
    "            \"total_earnings\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/ul/li[2]/span[2]/text()').get()\n",
    "            ,                        #/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/span/div/span\n",
    "            \"rating\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/span/div/span/text()').get()\n",
    "            ,                           #/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/span/div/span/span\n",
    "            \"nb_voters\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[1]/div[2]/div/div/span/div/span/span/text()').getall()[1]            \n",
    "        }\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"imdb1.json\"\n",
    "\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will \n",
    "# concatenate the last and new results otherwise)\n",
    "\n",
    "# A VERIFIER LUNDI\n",
    "#if filename in os.listdir('test_imdb1/'):\n",
    "       #os.remove('test_imdb1/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "## USER_AGENT => Simulates a browser on an OS\n",
    "## LOG_LEVEL => Minimal Level of Log \n",
    "## FEEDS => Where the file will be stored \n",
    "## More info on built-in settings => https://docs.scrapy.org/en/latest/topics/settings.html?highlight=settings#settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'imdb1/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "process.crawl(imdb_spider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a new script called `imdb2.py` where you'll scrape the same information for all the movies in the chart!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 16:03:46 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-01-20 16:03:47 [scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-5.4.129+-x86_64-with-glibc2.10\n",
      "2022-01-20 16:03:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-01-20 16:03:47 [scrapy.extensions.telnet] INFO: Telnet Password: 41c33caa114ee041\n",
      "2022-01-20 16:03:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-01-20 16:03:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-01-20 16:03:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-01-20 16:03:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-01-20 16:03:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-01-20 16:03:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-01-20 16:03:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-01-20 16:03:48 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-01-20 16:03:48 [scrapy.extensions.feedexport] INFO: Stored json feed (250 items) in: 01-Become_a_movie_director/imdb2.json\n",
      "2022-01-20 16:03:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 202,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 545783,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 1.764881,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 1, 20, 16, 3, 48, 860358),\n",
      " 'item_scraped_count': 250,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 51281920,\n",
      " 'memusage/startup': 51281920,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 1, 20, 16, 3, 47, 95477)}\n",
      "2022-01-20 16:03:48 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class imdb_spider(scrapy.Spider):\n",
    "\n",
    "    name = \"imdb\"\n",
    "\n",
    "    start_urls = [\n",
    "        'https://www.imdb.com/chart/top',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        n = 250\n",
    "        for i in range(n):\n",
    "            i = i + 1\n",
    "            yield {\n",
    "                \"duration\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[{}]/div[2]/div/div/div[2]/span[2]/text()'.format(i)).get(),\n",
    "                \"ranking\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[{}]/div[2]/div/div/div[1]/a/h3/text()'.format(i)).get().split(\".\")[0],\n",
    "                \"title\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[{}]/div[2]/div/div/div[1]/a/h3/text()'.format(i)).get().split(\".\")[1], \n",
    "                \"url\": response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[{}]/div[2]/div/div/div[1]/a'.format(i)).attrib[\"href\"],\n",
    "                \"rating\":response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[{}]/div[2]/div/div/span/div/span/text()'.format(i)).get(),\n",
    "                'year': response.xpath('/html/body/div[2]/main/div/div[3]/section/div/div[2]/div/ul/li[{}]/div[2]/div/div/div[2]/span[1]/text()'.format(i)).get()\n",
    "            }\n",
    "filename = \"imdb2.json\"\n",
    "\n",
    "if filename in os.listdir('test/'):\n",
    "        os.remove('test/' + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'test/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(imdb_spider)\n",
    "process.start()\n",
    "\n",
    "#rajouter du code python parser le code json et le transformer en DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Optional 💪💪\n",
    "6. Based on the results obtained, create a list called `url_list` containing all the urls for the movies in the charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ranking': '\\n      1.\\n      ',\n",
       "  'title': 'The Shawshank Redemption',\n",
       "  'url': '/title/tt0111161/',\n",
       "  'crew': 'Frank Darabont (dir.), Tim Robbins, Morgan Freeman',\n",
       "  'rating': '9.221177810079306',\n",
       "  'nb_voters': '2529631'},\n",
       " {'ranking': '\\n      2.\\n      ',\n",
       "  'title': 'The Godfather',\n",
       "  'url': '/title/tt0068646/',\n",
       "  'crew': 'Francis Ford Coppola (dir.), Marlon Brando, Al Pacino',\n",
       "  'rating': '9.14689710309463',\n",
       "  'nb_voters': '1741538'},\n",
       " {'ranking': '\\n      3.\\n      ',\n",
       "  'title': 'The Godfather: Part II',\n",
       "  'url': '/title/tt0071562/',\n",
       "  'crew': 'Francis Ford Coppola (dir.), Al Pacino, Robert De Niro',\n",
       "  'rating': '8.980975008069025',\n",
       "  'nb_voters': '1208299'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>crew</th>\n",
       "      <th>rating</th>\n",
       "      <th>nb_voters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n      1.\\n</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>/title/tt0111161/</td>\n",
       "      <td>Frank Darabont (dir.), Tim Robbins, Morgan Fre...</td>\n",
       "      <td>9.221177810079306</td>\n",
       "      <td>2529631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n      2.\\n</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>/title/tt0068646/</td>\n",
       "      <td>Francis Ford Coppola (dir.), Marlon Brando, Al...</td>\n",
       "      <td>9.14689710309463</td>\n",
       "      <td>1741538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n      3.\\n</td>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>/title/tt0071562/</td>\n",
       "      <td>Francis Ford Coppola (dir.), Al Pacino, Robert...</td>\n",
       "      <td>8.980975008069025</td>\n",
       "      <td>1208299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n      4.\\n</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>/title/tt0468569/</td>\n",
       "      <td>Christopher Nolan (dir.), Christian Bale, Heat...</td>\n",
       "      <td>8.976474723318388</td>\n",
       "      <td>2480096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n      5.\\n</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>/title/tt0050083/</td>\n",
       "      <td>Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb</td>\n",
       "      <td>8.942216792878675</td>\n",
       "      <td>747346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ranking                     title                url  \\\n",
       "0  \\n      1.\\n        The Shawshank Redemption  /title/tt0111161/   \n",
       "1  \\n      2.\\n                   The Godfather  /title/tt0068646/   \n",
       "2  \\n      3.\\n          The Godfather: Part II  /title/tt0071562/   \n",
       "3  \\n      4.\\n                 The Dark Knight  /title/tt0468569/   \n",
       "4  \\n      5.\\n                    12 Angry Men  /title/tt0050083/   \n",
       "\n",
       "                                                crew             rating  \\\n",
       "0  Frank Darabont (dir.), Tim Robbins, Morgan Fre...  9.221177810079306   \n",
       "1  Francis Ford Coppola (dir.), Marlon Brando, Al...   9.14689710309463   \n",
       "2  Francis Ford Coppola (dir.), Al Pacino, Robert...  8.980975008069025   \n",
       "3  Christopher Nolan (dir.), Christian Bale, Heat...  8.976474723318388   \n",
       "4      Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb  8.942216792878675   \n",
       "\n",
       "  nb_voters  \n",
       "0   2529631  \n",
       "1   1741538  \n",
       "2   1208299  \n",
       "3   2480096  \n",
       "4    747346  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url list: ['https://www.imdb.com/title/tt0111161/', 'https://www.imdb.com/title/tt0068646/', 'https://www.imdb.com/title/tt0071562/']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Store the list of urls to a file called `url_list.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Scrape all the pages from the movies in the charts and extract the complete cast, the Storyline, and the tags, also save the title and url for future joins purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 17:50:42 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-01-20 17:50:42 [scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 19:08:05) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.1.1, Platform Linux-5.4.129+-x86_64-with-glibc2.10\n",
      "2022-01-20 17:50:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-01-20 17:50:42 [scrapy.extensions.telnet] INFO: Telnet Password: da17500a43b1594c\n",
      "2022-01-20 17:50:42 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-01-20 17:50:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-01-20 17:50:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-01-20 17:50:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-01-20 17:50:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-01-20 17:50:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-01-20 17:50:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-01-20 17:51:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-01-20 17:51:15 [scrapy.extensions.feedexport] INFO: Stored json feed (250 items) in: 01-Become_a_movie_director/imdb3.json\n",
      "2022-01-20 17:51:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 52254,\n",
      " 'downloader/request_count': 250,\n",
      " 'downloader/request_method_count/GET': 250,\n",
      " 'downloader/response_bytes': 34305622,\n",
      " 'downloader/response_count': 250,\n",
      " 'downloader/response_status_count/200': 250,\n",
      " 'elapsed_time_seconds': 33.297829,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 1, 20, 17, 51, 15, 879160),\n",
      " 'httpcompression/response_bytes': 206801118,\n",
      " 'httpcompression/response_count': 250,\n",
      " 'item_scraped_count': 250,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 64561152,\n",
      " 'memusage/startup': 64561152,\n",
      " 'response_received_count': 250,\n",
      " 'scheduler/dequeued': 250,\n",
      " 'scheduler/dequeued/memory': 250,\n",
      " 'scheduler/enqueued': 250,\n",
      " 'scheduler/enqueued/memory': 250,\n",
      " 'start_time': datetime.datetime(2022, 1, 20, 17, 50, 42, 581331)}\n",
      "2022-01-20 17:51:15 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pay attention to inconsistencies between pages"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02-Become_movie_director_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "julie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "9296beafced54f20cfbf201aab49071c99f90dce4b7e44c77b4b7239373355a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
