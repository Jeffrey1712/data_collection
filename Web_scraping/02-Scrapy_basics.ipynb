{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy basics\n",
    "\n",
    "## What you will learn in this course 🧐🧐\n",
    "\n",
    "As you learned how to parse HTML pages, it is now time to go to the next level and scrape websites automatically. The best way to do so is by using spiders from Scrapy. In this course, we'll learn:\n",
    "\n",
    "* How to create basic crawlers \n",
    "* Target specific tags and attributes in a webpage \n",
    "* Follow links to scrap multiple pages\n",
    "* Simulate user log-in\n",
    "* Run multiple crawlers at the same time\n",
    "* Avoid being banned from websites\n",
    "\n",
    "If Scrapy isn't installed yet in your environment, just execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Scrapy\n",
      "  Downloading Scrapy-2.11.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Twisted>=18.9.0 (from Scrapy)\n",
      "  Downloading twisted-23.10.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting cryptography>=36.0.0 (from Scrapy)\n",
      "  Downloading cryptography-42.0.4-cp39-abi3-macosx_10_12_universal2.whl.metadata (5.3 kB)\n",
      "Collecting cssselect>=0.9.1 (from Scrapy)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting itemloaders>=1.0.1 (from Scrapy)\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from Scrapy)\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyOpenSSL>=21.0.0 (from Scrapy)\n",
      "  Downloading pyOpenSSL-24.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting queuelib>=1.4.2 (from Scrapy)\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0 (from Scrapy)\n",
      "  Downloading service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting w3lib>=1.17.0 (from Scrapy)\n",
      "  Downloading w3lib-2.1.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting zope.interface>=5.1.0 (from Scrapy)\n",
      "  Downloading zope.interface-6.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protego>=0.1.15 (from Scrapy)\n",
      "  Downloading Protego-0.3.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting itemadapter>=0.1.0 (from Scrapy)\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from Scrapy) (65.5.0)\n",
      "Requirement already satisfied: packaging in /Users/jeffreyensenat/Library/Python/3.11/lib/python/site-packages (from Scrapy) (23.2)\n",
      "Collecting tldextract (from Scrapy)\n",
      "  Downloading tldextract-5.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting lxml>=4.4.1 (from Scrapy)\n",
      "  Downloading lxml-5.1.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from Scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cryptography>=36.0.0->Scrapy) (1.16.0)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->Scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from service-identity>=18.1.0->Scrapy) (23.2.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->Scrapy)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->Scrapy)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting automat>=0.8.0 (from Twisted>=18.9.0->Scrapy)\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->Scrapy)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->Scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting incremental>=22.10.0 (from Twisted>=18.9.0->Scrapy)\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from Twisted>=18.9.0->Scrapy)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->Scrapy) (3.6)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tldextract->Scrapy) (2.31.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->Scrapy)\n",
      "  Downloading requests_file-2.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->Scrapy)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six in /Users/jeffreyensenat/Library/Python/3.11/lib/python/site-packages (from automat>=0.8.0->Twisted>=18.9.0->Scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->Scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->Scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->Scrapy) (2024.2.2)\n",
      "Downloading Scrapy-2.11.1-py2.py3-none-any.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-42.0.4-cp39-abi3-macosx_10_12_universal2.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading lxml-5.1.0-cp311-cp311-macosx_10_9_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
      "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Downloading pyOpenSSL-24.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Downloading twisted-23.10.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading zope.interface-6.2-cp311-cp311-macosx_10_9_x86_64.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyDispatcher, incremental, zope.interface, w3lib, typing-extensions, queuelib, pyasn1, protego, lxml, jmespath, itemadapter, hyperlink, filelock, cssselect, constantly, automat, Twisted, requests-file, pyasn1-modules, parsel, cryptography, tldextract, service-identity, pyOpenSSL, itemloaders, Scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Scrapy-2.11.1 Twisted-23.10.0 automat-22.10.0 constantly-23.10.4 cryptography-42.0.4 cssselect-1.2.0 filelock-3.13.1 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 lxml-5.1.0 parsel-1.8.1 protego-0.3.0 pyOpenSSL-24.0.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 queuelib-1.6.2 requests-file-2.0.0 service-identity-24.1.0 tldextract-5.1.1 typing-extensions-4.9.0 w3lib-2.1.2 zope.interface-6.2\n"
     ]
    }
   ],
   "source": [
    "# Add '!' only if you are running this command on a notebook \n",
    "## It tells Jupyter that the command should be interpreted as bash command\n",
    "!pip3 install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your first spider 🕷️🕷️\n",
    "\n",
    "Basically, Scrapy works with *Spiders* that describe the successive steps necessary to get the data you're interested in at a given url. To make a scraping engine, you will need to:\n",
    "\n",
    "- declare your own class that inherits from `Scrapy.Spider`,\n",
    "- declare two attributes: the `name` of your crawler and the `url` at which you will start crawling,\n",
    "- declare a `parse` method with an argument called `response` (which represents the variable containing the HTML response at the `url` you just defined).\n",
    "- The `response` object has ONE method that you ABSOLUTELY need to know and will help you get what you are looking for 95% of the time, it's called `.xpath()` and you will just have to copy an xpath from the webpage's source code to scrape the element, easy right ?!\n",
    "\n",
    "A Spider always looks somewhat like this:\n",
    "\n",
    "```python\n",
    "class RandomQuoteSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"mySpider\"\n",
    "\n",
    "    # Url to start your spider from \n",
    "    start_urls = [\n",
    "        'http://my.url.to.scrape',\n",
    "    ]\n",
    "\n",
    "    # Callback function that will be called when starting your spider\n",
    "    def parse(self, response):\n",
    "        return {\n",
    "            'result1': response.xpath(\"/some/xpath/\").get(),\n",
    "            'result2': response.xpath('/some/xpath/').get(),\n",
    "        }\n",
    "```\n",
    "\n",
    "Let's begin with a [simple example](src/scrapy1.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Crawler Process\n",
    "\n",
    "Once your spider has been set up, you have to declare a `CrawlerProcess` that will run the spider and save the results in a `json` file (called a \"FEED\").\n",
    "\n",
    "All you have to do is run the python script using `!python src/scrapy1.py` here in the notebook. Typically we do not write scraping code in the notebook but rather in scripts because it is then easier to use repeatedly (like everyday for example) or in an asynchronous manner (see optional lecture from module 4 day 1)\n",
    "\n",
    "The crawler process will always look like this:\n",
    "\n",
    "```python\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "```\n",
    "\n",
    "Let's study this in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User agent\n",
    "\n",
    "Scrapy is able to scrape the web by simulating a web browser (the client) that will send HTTP requests to a web server. The question is, what browser are you simulating? Ideally the browser you are simulating should be the same one that you are using to inspect the websites and get the XPath.\n",
    "\n",
    "The reason for this is that sometimes the webserver may give you different responses depending on the web browser you are using. For example old web browsers are not necessarily supporting javascript, which may cause the webpage to look a lot simpler and therefore change all the XPaths in the source code.\n",
    "\n",
    "In most cases, the user agent can be set like this:\n",
    "\n",
    "`'USER_AGENT' : 'Name_of_the_browser/version_number'`\n",
    "\n",
    "for example:\n",
    "\n",
    "`'USER_AGENT': 'Chrome/97.0'`\n",
    "\n",
    "If you are using chrome you should be able to find your browser version at [chrome://settings/help](chrome://settings/help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG LEVEL and FEED\n",
    "\n",
    "The other two settings we are giving the `CrawlerProcess` are the following:\n",
    "\n",
    "* `LOG_LEVEL`: which indicates what messages will be displayed in the logs, typically messages in the logs are classified in several levels such as CRITICAL, ERROR, WARNING, INFO, DEBUG... Choosing `logging.INFO` will display all the logs with importance INFO and higher.\n",
    "* `FEED`: indicates the destination and file format for the results to be saved.\n",
    "\n",
    "It is now time to run our first scraping code, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 19:24:05 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-15 19:24:05 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform Windows-10-10.0.22000-SP0\n",
      "2022-06-15 19:24:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-06-15 19:24:05 [scrapy.extensions.telnet] INFO: Telnet Password: efcded54877601a9\n",
      "2022-06-15 19:24:05 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-06-15 19:24:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-06-15 19:24:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-06-15 19:24:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-06-15 19:24:05 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-06-15 19:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-06-15 19:24:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-06-15 19:24:06 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-06-15 19:24:06 [scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: src/1_randomquote.json\n",
      "2022-06-15 19:24:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 206,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 835,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.43999,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 6, 15, 17, 24, 6, 251086),\n",
      " 'httpcompression/response_bytes': 2170,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 6, 15, 17, 24, 5, 811096)}\n",
      "2022-06-15 19:24:06 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!python src/scrapy1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may take a look at the result in [this file](src/1_randomquote.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: Scrapy is not made to run multiple independant crawlers in one script. Therefore each script will contain a single Crawler. This also why we do not use scrapy within the notebook, this is not the usage it was designed for. Plus it will make you practice writing scripts instead of notebooks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple items per page 🛍️🛍️\n",
    "\n",
    "Let's see an example where we parse multiple elements with a `for` loop and python's `yield` instruction (see appendix 1 of this lecture for details):\n",
    "\n",
    "If you take a look at the following [webpage](http://quotes.toscrape.com/page/1/), you may see that lots of quoes are available. Let's take a look at the XPath for the first quote:\n",
    "\n",
    "`/html/body/div/div[2]/div[1]/div[1]/span[1]/text()`\n",
    "\n",
    "Now let's take a look at the XPath for the second quote:\n",
    "\n",
    "`/html/body/div/div[2]/div[1]/div[2]/span[1]/text()` \n",
    "\n",
    "We can see that only the index of the 4th `div` tag is changing, therefore the general XPath for the quotes is:\n",
    "\n",
    "`/html/body/div/div[2]/div[1]/div/span[1]/text()`\n",
    "\n",
    "We could take advantage of this or we could loop until the last element which XPath is:\n",
    "\n",
    "`/html/body/div/div[2]/div[1]/div[10]/span[1]/text()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 19:24:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-15 19:24:07 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform Windows-10-10.0.22000-SP0\n",
      "2022-06-15 19:24:07 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-06-15 19:24:07 [scrapy.extensions.telnet] INFO: Telnet Password: 270f70707f21c012\n",
      "2022-06-15 19:24:07 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-06-15 19:24:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-06-15 19:24:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-06-15 19:24:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-06-15 19:24:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-06-15 19:24:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-06-15 19:24:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-06-15 19:24:07 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-06-15 19:24:07 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: src/2_quotes.json\n",
      "2022-06-15 19:24:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 207,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2086,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.55903,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 6, 15, 17, 24, 7, 877119),\n",
      " 'httpcompression/response_bytes': 11053,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 6, 15, 17, 24, 7, 318089)}\n",
      "2022-06-15 19:24:07 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!python src/scrapy2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 19:24:09 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-06-15 19:24:09 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform Windows-10-10.0.22000-SP0\n",
      "2022-06-15 19:24:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-06-15 19:24:09 [scrapy.extensions.telnet] INFO: Telnet Password: f8c011e1676a05ba\n",
      "2022-06-15 19:24:09 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-06-15 19:24:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-06-15 19:24:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-06-15 19:24:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-06-15 19:24:09 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-06-15 19:24:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-06-15 19:24:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-06-15 19:24:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-06-15 19:24:10 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: src/2_quotes_alt.json\n",
      "2022-06-15 19:24:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 207,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2087,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.699869,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 6, 15, 17, 24, 10, 160429),\n",
      " 'httpcompression/response_bytes': 11053,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'item_scraped_count': 10,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 6, 15, 17, 24, 9, 460560)}\n",
      "2022-06-15 19:24:10 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!python src/scrapy2-alt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 1 - What is Yield keyword for? 💐\n",
    "\n",
    "You might have noticed that we used the `yield` keyword in Scrapy which could be quite new and confusing. Technically speaking it is called a *generator*.\n",
    "\n",
    "In a nutshell, `yield` is a very useful keyword to return a data collection without taking up too much machine's memory. \n",
    "\n",
    "Let's check out with an example. Let's take two functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function with return keyword\n",
    "def return_list(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        a_list[i] *= 2\n",
    "    return a_list\n",
    "\n",
    "# Function with yield keyword\n",
    "def return_with_yield(a_list):\n",
    "    for i in range(len(a_list)):\n",
    "        yield a_list[i] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply these two functions to our `random_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of numbers from 0 to 9\n",
    "random_list = [x for x in range(10)]\n",
    "# Returns a list\n",
    "return_list(random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object return_with_yield at 0x0000022E0DABCAC0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of numbers from 0 to 9\n",
    "random_list = [x for x in range(10)]\n",
    "# Function with yield\n",
    "return_with_yield(random_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example, `return_list` returned directly the full list. Whereas, in the second example, `return_with_yield` returned a `generator`. Generators are very cool because we haven't actually executed the loop. Therefore, we haven't spend too much computer memory. \n",
    "\n",
    "So let's say instead of a list of 10 items, you'd have one of 1000000 items, it would make a huge difference in terms of computing time. \n",
    "\n",
    "Now if you need to get the actual values of your generator, you can simply create a for loop or a comprehension list like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 0\n",
      "output 2\n",
      "output 4\n",
      "output 6\n",
      "output 8\n",
      "output 10\n",
      "output 12\n",
      "output 14\n",
      "output 16\n",
      "output 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a for loop will just print the output:\n",
    "for number in return_with_yield(random_list):\n",
    "    print(\"output\", number)\n",
    "\n",
    "# Using a comprehension list will create a list:\n",
    "[i for i in return_with_yield(random_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you simply need to yield from a list without doing any manipulation, you can use `yield from` instead of creating a loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix 2 - Crash course on XPath ⚔️\n",
    "\n",
    "The best way to learn XPath is to follow this great tutorial from <a href=\"http://zvon.org/comp/r/tut-XPath_1.html#Pages~List_of_XPaths\" target=\"_blank\">http://Zvon.org</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources 📚📚\n",
    "\n",
    "* <a href=\"https://docs.scrapy.org/en/latest/index.html\" target=\"_blank\"> Scrapy Documentation </a>\n",
    "* <a href=\"https://docs.python.org/3/library/logging.html\" target=\"_blank\"> Logging</a>\n",
    "* <a href=\"https://docs.scrapy.org/en/latest/topics/logging.html#topics-logging\" target=\"_blank\">Logging in a scrapy</a>\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16196ea7eff63910081d4e10ae1bdb1eb18fd83cb470bb8efbb9fa6b0c724af5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
